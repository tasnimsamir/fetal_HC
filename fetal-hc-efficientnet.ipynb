{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Libraries**"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-01-12T20:38:40.501501Z","iopub.status.busy":"2021-01-12T20:38:40.500632Z","iopub.status.idle":"2021-01-12T20:38:40.503812Z","shell.execute_reply":"2021-01-12T20:38:40.503302Z"},"papermill":{"duration":0.023901,"end_time":"2021-01-12T20:38:40.503934","exception":false,"start_time":"2021-01-12T20:38:40.480033","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:38:40.538991Z","iopub.status.busy":"2021-01-12T20:38:40.538348Z","iopub.status.idle":"2021-01-12T20:38:43.137159Z","shell.execute_reply":"2021-01-12T20:38:43.136399Z"},"papermill":{"duration":2.618939,"end_time":"2021-01-12T20:38:43.13727","exception":false,"start_time":"2021-01-12T20:38:40.518331","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# PyTorch\nimport torch\nfrom torchvision import transforms, datasets, models\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torch.nn as nn\nfrom torch import optim, cuda\n\n# Visualizations\nimport matplotlib.pyplot as plt\nfrom skimage import io, transform\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode\n\n# Timing utility\nfrom timeit import default_timer as timer\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Variables**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:38:43.521767Z","iopub.status.busy":"2021-01-12T20:38:43.521095Z","iopub.status.idle":"2021-01-12T20:38:43.526045Z","shell.execute_reply":"2021-01-12T20:38:43.526643Z"},"papermill":{"duration":0.375294,"end_time":"2021-01-12T20:38:43.526771","exception":false,"start_time":"2021-01-12T20:38:43.151477","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"save_file_name = 'efficientnet-transfer-4.pt'\ncheckpoint_path = 'efficientnet-transfer-4.pth'\nearlyStop=300\nepochsNo=300\n\nroot_dir='/kaggle/input/hc18-ultrasound/HC18/training_set/training_set/'\ncsv_file='/kaggle/input/hc18-ultrasound/HC18/training_set_pixel_size_and_HC.csv'\nimg_size = 224\ncropped_img = 224\nbatch_size = 10\n\n# Whether to train on a gpu\ntrain_on_gpu = cuda.is_available()\nprint(f'Train on gpu: {train_on_gpu}')\n\n# Number of gpus\nif train_on_gpu:\n    gpu_count = cuda.device_count()\n    print(f'{gpu_count} gpus detected.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Analysis & Manipulation**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-01-12T20:38:43.575724Z","iopub.status.busy":"2021-01-12T20:38:43.575121Z","iopub.status.idle":"2021-01-12T20:38:43.783784Z","shell.execute_reply":"2021-01-12T20:38:43.784411Z"},"papermill":{"duration":0.242318,"end_time":"2021-01-12T20:38:43.78456","exception":false,"start_time":"2021-01-12T20:38:43.542242","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"HC_df=pd.read_csv(os.path.join(csv_file))\n\nfrom sklearn.model_selection import train_test_split\nHC_df = HC_df[['filename','pixel size(mm)','head circumference (mm)']].drop_duplicates()\nHC_df=HC_df[['filename','pixel size(mm)','head circumference (mm)']].copy().rename(columns={'pixel size(mm)':'pixel_size','head circumference (mm)':'HC'})\n\n# HC_df['HC_pixels'] = HC_df['HC'] / HC_df['pixel_size']\n# HC_df['normalized_HC_pixels'] = HC_df['HC_pixels'] / 1786.500242\nHC_df = HC_df[HC_df.filename.str.contains(\"_HC\")]\n\ntrain_df, test_val = train_test_split(HC_df, test_size = 0.3,random_state = 2020)\nvalid_df, test_df = train_test_split(test_val, test_size = 0.5,random_state = 2020)\n\nprint('train', train_df.shape[0], 'validation', valid_df.shape[0],'Test',test_df.shape[0])\n# train_df = train_df.groupby(['pixel_size', 'HC']).apply(lambda x: x.sample(4, replace = True)).reset_index(drop = True)\n# print('train', train_df.shape[0], 'validation', valid_df.shape[0],'Test',test_df.shape[0])\n\nHC_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:38:43.823014Z","iopub.status.busy":"2021-01-12T20:38:43.822293Z","iopub.status.idle":"2021-01-12T20:38:44.148763Z","shell.execute_reply":"2021-01-12T20:38:44.149271Z"},"papermill":{"duration":0.348333,"end_time":"2021-01-12T20:38:44.149409","exception":false,"start_time":"2021-01-12T20:38:43.801076","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":" # plot a histogram  \nHC_df['HC'].hist(bins=10)\ncolumns = ['HC']\nHC_df[columns].plot.box()\nplt.xticks(rotation='vertical')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:38:44.19021Z","iopub.status.busy":"2021-01-12T20:38:44.189516Z","iopub.status.idle":"2021-01-12T20:38:44.440799Z","shell.execute_reply":"2021-01-12T20:38:44.440258Z"},"papermill":{"duration":0.273443,"end_time":"2021-01-12T20:38:44.440939","exception":false,"start_time":"2021-01-12T20:38:44.167496","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train_df.to_csv(\"/kaggle/working/train_file.csv\", index=False, encoding='utf8')\n\nvalid_df.to_csv(\"/kaggle/working/val_file.csv\", index=False, encoding='utf8')\n\ntest_df.to_csv(\"/kaggle/working/test_file.csv\", index=False, encoding='utf8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Custom HC18 Dataset Class**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:38:44.486193Z","iopub.status.busy":"2021-01-12T20:38:44.485419Z","iopub.status.idle":"2021-01-12T20:38:44.488388Z","shell.execute_reply":"2021-01-12T20:38:44.487924Z"},"papermill":{"duration":0.029847,"end_time":"2021-01-12T20:38:44.488481","exception":false,"start_time":"2021-01-12T20:38:44.458634","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# Image manipulations\nfrom PIL import Image\n\nclass HC_18(Dataset):\n    \"\"\"Head circumference estimation dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.HC_df = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.HC_df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir,self.HC_df.iloc[idx, 0])\n        image = Image.open(img_name)\n        features = self.HC_df.loc[idx,'HC']\n        \n        if self.transform:\n            image = self.transform(image)\n        return image , torch.tensor(features / 346.4)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Images Transformations**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:38:44.535381Z","iopub.status.busy":"2021-01-12T20:38:44.534178Z","iopub.status.idle":"2021-01-12T20:38:44.536427Z","shell.execute_reply":"2021-01-12T20:38:44.53691Z"},"papermill":{"duration":0.03139,"end_time":"2021-01-12T20:38:44.537016","exception":false,"start_time":"2021-01-12T20:38:44.505626","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"image_transforms = {\n    # Train uses data augmentation\n    'train':\n        transforms.Compose([\n            transforms.Grayscale(num_output_channels=3),\n            transforms.Resize(size=img_size),\n            transforms.RandomRotation(degrees=10),\n            transforms.GaussianBlur(5),\n            transforms.RandomHorizontalFlip(),\n            transforms.CenterCrop(size=cropped_img),  # Image net standards\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n\n    # Validation does not use augmentation\n    'val':\n        transforms.Compose([\n            transforms.Grayscale(num_output_channels=3),\n            transforms.Resize(size=img_size),\n            transforms.GaussianBlur(5),\n            transforms.CenterCrop(size=cropped_img),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n    # Test does not use augmentation\n    'test':\n        transforms.Compose([\n            transforms.Grayscale(num_output_channels=3),\n            transforms.Resize(size=img_size),\n            transforms.GaussianBlur(5),\n            transforms.CenterCrop(size=cropped_img),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Dataloader**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:38:44.580602Z","iopub.status.busy":"2021-01-12T20:38:44.579746Z","iopub.status.idle":"2021-01-12T20:38:44.589195Z","shell.execute_reply":"2021-01-12T20:38:44.588546Z"},"papermill":{"duration":0.034715,"end_time":"2021-01-12T20:38:44.589287","exception":false,"start_time":"2021-01-12T20:38:44.554572","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train_filename='train_file.csv'\nval_filename='val_file.csv'\ntest_filename='test_file.csv'\n\ndata = {'train': HC_18(train_filename ,root_dir = root_dir,transform = image_transforms['train']),\n        'val': HC_18(val_filename ,root_dir = root_dir,transform = image_transforms['val']),\n        'test': HC_18(test_filename ,root_dir = root_dir,transform = image_transforms['test'])\n       }\n\n# Dataloader iterators\ndataloaders ={\n    'train': DataLoader(data['train'], batch_size=batch_size,shuffle=True),\n    'val': DataLoader(data['val'], batch_size=batch_size,shuffle=False),\n    'test': DataLoader(data['test'], batch_size=len(test_df),shuffle=False)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:38:45.830241Z","iopub.status.busy":"2021-01-12T20:38:45.829247Z","iopub.status.idle":"2021-01-12T20:39:32.968294Z","shell.execute_reply":"2021-01-12T20:39:32.967151Z"},"papermill":{"duration":47.212976,"end_time":"2021-01-12T20:39:32.968429","exception":false,"start_time":"2021-01-12T20:38:45.755453","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"!pip install efficientnet_pytorch\n\nfrom efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_pretrained('efficientnet-b7')\n\n\nn_inputs = model._fc.in_features\nmodel._fc = nn.Sequential(\n    nn.Linear(n_inputs, 1 , bias = True)   \n)\n\n# print(model)\n\nif train_on_gpu:\n    model = model.to('cuda')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Training Function**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:39:33.072719Z","iopub.status.busy":"2021-01-12T20:39:33.070904Z","iopub.status.idle":"2021-01-12T20:39:33.073394Z","shell.execute_reply":"2021-01-12T20:39:33.073863Z"},"papermill":{"duration":0.065885,"end_time":"2021-01-12T20:39:33.073977","exception":false,"start_time":"2021-01-12T20:39:33.008092","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# --------------------------------------Training\ndef train(model,criterion,optimizer,train_loader,valid_loader,save_file_name,max_epochs_stop=3,n_epochs=20,print_every=1):\n\n    # Early stopping intialization\n    epochs_no_improve = 0\n    valid_loss_min = np.Inf\n    valid_max_acc = 0\n    history = []\n    # Number of epochs already trained (if using loaded in model weights)\n    try:\n        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n    except:\n        model.epochs = 0\n        print(f'Starting Training from Scratch...\\n')\n\n    overall_start = timer()\n\n    # Main loop\n    for epoch in range(n_epochs):\n        # keep track of training and validation loss each epoch\n        train_loss = 0.0\n        valid_loss = 0.0\n        # Set to training\n        model.train()\n        start = timer()\n\n        # Training loop\n        for ii, (data, target) in enumerate(train_loader):\n            # Tensors to gpu\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n\n            # Clear gradients\n            optimizer.zero_grad()\n            # Predicted outputs are log probabilities\n            output = model(data)\n            # Loss and backpropagation of gradients\n            loss = criterion(output, target.float())\n            loss.backward()\n\n            # Update the parameters\n            optimizer.step()\n\n            # Track train loss by multiplying average loss by number of examples in batch\n            train_loss += loss.item() * data.size(0)\n\n            # Track training progress\n            print( f'Epoch: {epoch+1}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',end='\\r')\n\n        # After training loops ends, start validation\n        else:\n            model.epochs += 1\n\n            # Don't need to keep track of gradients\n            with torch.no_grad():\n                # Set to evaluation mode\n                model.eval()\n\n                # Validation loop\n                for data, target in valid_loader:\n                    # Tensors to gpu\n                    if train_on_gpu:\n                        data, target = data.cuda(), target.cuda()\n\n                    # Forward pass\n                    output = model(data)\n\n                    # Validation loss\n                    loss = criterion(output, target.float())\n                    # Multiply average loss times the number of examples in batch\n                    valid_loss += loss.item() * data.size(0)\n                    \n                # Calculate average losses\n                train_loss = train_loss / len(train_loader.dataset)\n                valid_loss = valid_loss / len(valid_loader.dataset)\n\n                history.append([train_loss, valid_loss])\n                # Print training and validation results\n                if (epoch + 1) % print_every == 0:\n                    print(f'\\nEpoch: {epoch+1} \\tTraining Loss: {train_loss:.4f} \\t\\tValidation Loss: {valid_loss:.4f}')\n\n                # Save the model if validation loss decreases\n                if valid_loss < valid_loss_min:\n                    # Save model\n                    torch.save(model.state_dict(), save_file_name)\n                    # Track improvement\n                    epochs_no_improve = 0\n                    valid_loss_min = valid_loss\n                    best_epoch = epoch\n\n                # Otherwise increment count of epochs with no improvement\n                else:\n                    epochs_no_improve += 1\n                    # Trigger early stopping\n                    if epochs_no_improve >= max_epochs_stop:\n                        print(f'\\nEarly Stopping! Total epochs: {epoch+1}. Best epoch: {best_epoch+1} with loss: {valid_loss_min:.2f}')\n                        total_time = timer() - overall_start\n                        print(f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.')\n\n                        # Load the best state dict\n                        model.load_state_dict(torch.load(save_file_name))\n                        # Attach the optimizer\n                        model.optimizer = optimizer\n\n                        # Format history\n                        history = pd.DataFrame(history,columns=['train_loss', 'valid_loss'])\n                        return model, history\n\n    # Attach the optimizer\n    model.optimizer = optimizer\n    # Record overall time and print out stats\n    total_time = timer() - overall_start\n    print(f'\\nBest epoch: {best_epoch+1} with loss: {valid_loss_min:.2f}')\n    print(f'{total_time:.2f} total seconds elapsed. {total_time / (epoch + 1):.2f} seconds per epoch.')\n    # Format history\n    history = pd.DataFrame(history,columns=['train_loss', 'valid_loss'])\n    return model, history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loss Functions & Optimizer**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:39:33.170499Z","iopub.status.busy":"2021-01-12T20:39:33.168396Z","iopub.status.idle":"2021-01-12T20:39:33.171233Z","shell.execute_reply":"2021-01-12T20:39:33.171706Z"},"papermill":{"duration":0.059905,"end_time":"2021-01-12T20:39:33.171818","exception":false,"start_time":"2021-01-12T20:39:33.111913","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# --------------------------------------Optimizer\ncriterion=nn.MSELoss()\n# criterion = nn.L1Loss()\n# criterion = nn.SmoothL1Loss(beta = 0.5)\noptimizer = optim.Adam(model.parameters(),lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Training..**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T20:39:33.261565Z","iopub.status.busy":"2021-01-12T20:39:33.260729Z","iopub.status.idle":"2021-01-12T23:47:28.599569Z","shell.execute_reply":"2021-01-12T23:47:28.600351Z"},"papermill":{"duration":11275.390832,"end_time":"2021-01-12T23:47:28.600589","exception":false,"start_time":"2021-01-12T20:39:33.209757","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# --------------------------------------Training\nmodel, history = train(model,criterion,optimizer,dataloaders['train'],dataloaders['val'],save_file_name=save_file_name,max_epochs_stop=earlyStop,n_epochs=epochsNo,print_every=1)\nhistory.to_csv(\"/kaggle/working/history.csv\", index=False, encoding='utf8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Results Graph**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T23:47:39.52574Z","iopub.status.busy":"2021-01-12T23:47:39.522282Z","iopub.status.idle":"2021-01-12T23:47:39.696384Z","shell.execute_reply":"2021-01-12T23:47:39.696873Z"},"papermill":{"duration":5.511084,"end_time":"2021-01-12T23:47:39.697001","exception":false,"start_time":"2021-01-12T23:47:34.185917","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nfor c in ['train_loss', 'valid_loss']:\n    plt.plot(history[c], label=c)\nplt.legend()\nplt.xlabel('Epoch',color='k')\nplt.ylabel('Average Negative Log Likelihood')\nplt.title('Training and Validation Losses')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Save Model**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T23:47:50.010641Z","iopub.status.busy":"2021-01-12T23:47:50.009247Z","iopub.status.idle":"2021-01-12T23:47:50.011286Z","shell.execute_reply":"2021-01-12T23:47:50.011753Z"},"papermill":{"duration":5.311911,"end_time":"2021-01-12T23:47:50.01189","exception":false,"start_time":"2021-01-12T23:47:44.699979","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"def save_checkpoint(model, path):\n\n    # Basic details\n    checkpoint = {\n                'epochs': model.epochs\n                 }\n\n    # Extract the final classifier and the state dictionary\n    checkpoint['fc'] = model._fc\n    checkpoint['state_dict'] = model.state_dict()\n\n    # Add the optimizer\n    checkpoint['optimizer'] = model.optimizer\n    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()\n    # Save the data to the path\n    torch.save(checkpoint, path)\n    ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T23:48:01.514894Z","iopub.status.busy":"2021-01-12T23:48:01.484416Z","iopub.status.idle":"2021-01-12T23:48:03.525162Z","shell.execute_reply":"2021-01-12T23:48:03.523953Z"},"papermill":{"duration":7.970083,"end_time":"2021-01-12T23:48:03.525299","exception":false,"start_time":"2021-01-12T23:47:55.555216","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"save_checkpoint(model, path=checkpoint_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Test the Model**"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T23:48:13.853574Z","iopub.status.busy":"2021-01-12T23:48:13.85252Z","iopub.status.idle":"2021-01-12T23:48:13.856191Z","shell.execute_reply":"2021-01-12T23:48:13.855534Z"},"papermill":{"duration":5.301957,"end_time":"2021-01-12T23:48:13.856316","exception":false,"start_time":"2021-01-12T23:48:08.554359","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"\ndef check_mse_on_test(model, testloader, criterion=None, device='cpu'):  \n    \"\"\"\n    Compute the Mean Square Error on the test dataset\n    :param model - a pretrained model object\n    :param testloader - a generator object representing the test dataset\n    :param criterion - loss object\n    :param device - a string specifying whether to use cuda or cpu\n    return - MSE of predicted head circumference\n    \"\"\"\n    loss = 0\n    test_loss = 0\n    std_mean = 0\n    model.eval()\n    test_dataframe = pd.read_csv(test_filename)\n    with torch.no_grad():\n        for data,target in testloader:\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n            output = model(data)\n\n            test_dataframe ['predicted output'] = np.squeeze(output.cpu().numpy())\n            test_dataframe.to_csv(\"/kaggle/working/pred_test_file.csv\", index=False, encoding='utf8')\n\n            std_mean = torch.std_mean(output)\n            loss = criterion(output, target.float())\n            # Multiply average loss times the number of examples in batch\n            test_loss += loss.item() * data.size(0)\n            \n    return std_mean , test_loss / len(testloader.dataset)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T23:48:24.772121Z","iopub.status.busy":"2021-01-12T23:48:24.770111Z","iopub.status.idle":"2021-01-12T23:48:24.772821Z","shell.execute_reply":"2021-01-12T23:48:24.773316Z"},"papermill":{"duration":5.812043,"end_time":"2021-01-12T23:48:24.773446","exception":false,"start_time":"2021-01-12T23:48:18.961403","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"criterion = nn.L1Loss()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-12T23:48:35.618482Z","iopub.status.busy":"2021-01-12T23:48:35.61787Z","iopub.status.idle":"2021-01-12T23:48:40.196007Z","shell.execute_reply":"2021-01-12T23:48:40.195508Z"},"papermill":{"duration":10.302787,"end_time":"2021-01-12T23:48:40.19614","exception":false,"start_time":"2021-01-12T23:48:29.893353","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"std_mean , test_loss = check_mse_on_test(model, dataloaders['test'], criterion, device='cuda')\nprint('Test set MAE loss = {:.4f} '.format(test_loss))\nprint('Test set standard deviation of Hc =  {:.4f} mm'.format(std_mean[0] * 346.4))\nprint('Test set MAE loss of HC = {:.4f} mm'.format(test_loss * 346.4))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}