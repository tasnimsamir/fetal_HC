{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PyTorch\nimport torch\nfrom torchvision import transforms, datasets, models\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torch.nn as nn\nfrom torch import optim, cuda\n\n# Visualizations\nimport matplotlib.pyplot as plt\nfrom skimage import io, transform\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode\n\n# Timing utility\nfrom timeit import default_timer as timer\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"save_file_name = 'resnet50-transfer-4.pt'\ncheckpoint_path = 'resnet50-transfer-4.pth'\nearlyStop=300\nepochsNo=300\n\nroot_dir='/kaggle/input/hc18-ultrasound/HC18/training_set/training_set/'\ncsv_file='/kaggle/input/hc18-ultrasound/HC18/training_set_pixel_size_and_HC.csv'\nimg_size = 224\ncropped_img = 224\nbatch_size = 100\n\n# Whether to train on a gpu\ntrain_on_gpu = cuda.is_available()\nprint(f'Train on gpu: {train_on_gpu}')\n\n# Number of gpus\nif train_on_gpu:\n    gpu_count = cuda.device_count()\n    print(f'{gpu_count} gpus detected.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Analysis & Manipulation**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"HC_df=pd.read_csv(os.path.join(csv_file))\n\nfrom sklearn.model_selection import train_test_split\nHC_df = HC_df[['filename','pixel size(mm)','head circumference (mm)']].drop_duplicates()\nHC_df=HC_df[['filename','pixel size(mm)','head circumference (mm)']].copy().rename(columns={'pixel size(mm)':'pixel_size','head circumference (mm)':'HC'})\n\n# HC_df['HC_pixels'] = HC_df['HC'] / HC_df['pixel_size']\n# HC_df['normalized_HC_pixels'] = HC_df['HC_pixels'] / 1786.500242\nHC_df = HC_df[HC_df.filename.str.contains(\"_HC\")]\n\ntrain_df, test_val = train_test_split(HC_df, test_size = 0.3)\nvalid_df, test_df = train_test_split(test_val, test_size = 0.5)\n\nprint('train', train_df.shape[0], 'validation', valid_df.shape[0],'Test',test_df.shape[0])\n# train_df = train_df.groupby(['pixel_size', 'HC']).apply(lambda x: x.sample(4, replace = True)).reset_index(drop = True)\n# print('train', train_df.shape[0], 'validation', valid_df.shape[0],'Test',test_df.shape[0])\n\nHC_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # plot a histogram  \nHC_df['HC'].hist(bins=10)\ncolumns = ['HC']\nHC_df[columns].plot.box()\nplt.xticks(rotation='vertical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.to_csv(\"/kaggle/working/train_file.csv\", index=False, encoding='utf8')\n\nvalid_df.to_csv(\"/kaggle/working/val_file.csv\", index=False, encoding='utf8')\n\ntest_df.to_csv(\"/kaggle/working/test_file.csv\", index=False, encoding='utf8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Custom HC18 Class**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image manipulations\nfrom PIL import Image\n\nclass HC_18(Dataset):\n    \"\"\"Head circumference estimation dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.HC_df = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.HC_df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        img_name = os.path.join(self.root_dir,self.HC_df.iloc[idx, 0])\n        image = Image.open(img_name)\n        features = self.HC_df.loc[idx,'HC']\n        \n        if self.transform:\n            image = self.transform(image)\n        return image , torch.tensor(features / 346.4)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Image Transformations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_transforms = {\n    # Train uses data augmentation\n    'train':\n        transforms.Compose([\n            transforms.Grayscale(num_output_channels=3),\n            transforms.Resize(size=img_size),\n            transforms.RandomRotation(degrees=10),\n#             transforms.GaussianBlur(5),\n            transforms.RandomHorizontalFlip(),\n            transforms.CenterCrop(size=cropped_img),  # Image net standards\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n\n    # Validation does not use augmentation\n    'val':\n        transforms.Compose([\n            transforms.Grayscale(num_output_channels=3),\n            transforms.Resize(size=img_size),\n            transforms.CenterCrop(size=cropped_img),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n    # Test does not use augmentation\n    'test':\n        transforms.Compose([\n            transforms.Grayscale(num_output_channels=3),\n            transforms.Resize(size=img_size),\n            transforms.CenterCrop(size=cropped_img),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Dataloader**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_filename='train_file.csv'\nval_filename='val_file.csv'\ntest_filename='test_file.csv'\n\ndata = {'train': HC_18(train_filename ,root_dir = root_dir,transform = image_transforms['train']),\n        'val': HC_18(val_filename ,root_dir = root_dir,transform = image_transforms['val']),\n        'test': HC_18(test_filename ,root_dir = root_dir,transform = image_transforms['test'])\n       }\n\n# Dataloader iterators\ndataloaders ={\n    'train': DataLoader(data['train'], batch_size=batch_size,shuffle=True),\n    'val': DataLoader(data['val'], batch_size=batch_size,shuffle=False),\n    'test': DataLoader(data['test'], batch_size=len(test_df),shuffle=False)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainiter = iter(dataloaders['train'])\nt_x, t_y = next(trainiter)\nprint(t_x.shape, t_y.shape)\nfig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n    trans=transforms.ToPILImage(mode= None)\n    c_ax.imshow(trans(c_x))\n#     c_y = np.exp(c_y.numpy())\n    c_ax.set_title('HC = {:.4f}'.format(c_y.numpy()))\n    c_ax.axis('off')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.resnet50(pretrained=True)\nn_inputs = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Dropout(0.2),\n    nn.Linear(n_inputs, 1 , bias = True)   \n)\n\nprint(model)\n\nif train_on_gpu:\n    model = model.to('cuda')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Training Function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------Training\ndef train(model,criterion,optimizer,train_loader,valid_loader,save_file_name,max_epochs_stop=3,n_epochs=20,print_every=1):\n\n    # Early stopping intialization\n    epochs_no_improve = 0\n    valid_loss_min = np.Inf\n    valid_max_acc = 0\n    history = []\n    # Number of epochs already trained (if using loaded in model weights)\n    try:\n        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n    except:\n        model.epochs = 0\n        print(f'Starting Training from Scratch...\\n')\n\n    overall_start = timer()\n\n    # Main loop\n    for epoch in range(n_epochs):\n        # keep track of training and validation loss each epoch\n        train_loss = 0.0\n        valid_loss = 0.0\n        # Set to training\n        model.train()\n        start = timer()\n\n        # Training loop\n        for ii, (data, target) in enumerate(train_loader):\n            # Tensors to gpu\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n\n            # Clear gradients\n            optimizer.zero_grad()\n            # Predicted outputs are log probabilities\n            output = model(data)\n            # Loss and backpropagation of gradients\n            loss = criterion(output, target.float())\n            loss.backward()\n\n            # Update the parameters\n            optimizer.step()\n\n            # Track train loss by multiplying average loss by number of examples in batch\n            train_loss += loss.item() * data.size(0)\n\n            # Track training progress\n            print( f'Epoch: {epoch+1}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',end='\\r')\n\n        # After training loops ends, start validation\n        else:\n            model.epochs += 1\n\n            # Don't need to keep track of gradients\n            with torch.no_grad():\n                # Set to evaluation mode\n                model.eval()\n\n                # Validation loop\n                for data, target in valid_loader:\n                    # Tensors to gpu\n                    if train_on_gpu:\n                        data, target = data.cuda(), target.cuda()\n\n                    # Forward pass\n                    output = model(data)\n\n                    # Validation loss\n                    loss = criterion(output, target.float())\n                    # Multiply average loss times the number of examples in batch\n                    valid_loss += loss.item() * data.size(0)\n                    \n                # Calculate average losses\n                train_loss = train_loss / len(train_loader.dataset)\n                valid_loss = valid_loss / len(valid_loader.dataset)\n\n                history.append([train_loss, valid_loss])\n                # Print training and validation results\n                if (epoch + 1) % print_every == 0:\n                    print(f'\\nEpoch: {epoch+1} \\tTraining Loss: {train_loss:.4f} \\t\\tValidation Loss: {valid_loss:.4f}')\n\n                # Save the model if validation loss decreases\n                if valid_loss < valid_loss_min:\n                    # Save model\n                    torch.save(model.state_dict(), save_file_name)\n                    # Track improvement\n                    epochs_no_improve = 0\n                    valid_loss_min = valid_loss\n                    best_epoch = epoch\n\n                # Otherwise increment count of epochs with no improvement\n                else:\n                    epochs_no_improve += 1\n                    # Trigger early stopping\n                    if epochs_no_improve >= max_epochs_stop:\n                        print(f'\\nEarly Stopping! Total epochs: {epoch+1}. Best epoch: {best_epoch+1} with loss: {valid_loss_min:.2f}')\n                        total_time = timer() - overall_start\n                        print(f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.')\n\n                        # Load the best state dict\n                        model.load_state_dict(torch.load(save_file_name))\n                        # Attach the optimizer\n                        model.optimizer = optimizer\n\n                        # Format history\n                        history = pd.DataFrame(history,columns=['train_loss', 'valid_loss'])\n                        return model, history\n\n    # Attach the optimizer\n    model.optimizer = optimizer\n    # Record overall time and print out stats\n    total_time = timer() - overall_start\n    print(f'\\nBest epoch: {best_epoch+1} with loss: {valid_loss_min:.2f}')\n    print(f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.')\n    # Format history\n    history = pd.DataFrame(history,columns=['train_loss', 'valid_loss'])\n    return model, history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loss Function & Optimizer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------Optimizer\ncriterion=nn.MSELoss()\n# criterion = nn.L1Loss()\n# criterion = nn.SmoothL1Loss(beta = 0.5)\noptimizer = optim.Adam(model.parameters(),lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Training...**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------------------Training\nmodel, history = train(model,criterion,optimizer,dataloaders['train'],dataloaders['val'],save_file_name=save_file_name,max_epochs_stop=earlyStop,n_epochs=epochsNo,print_every=1)\nhistory.to_csv(\"/kaggle/working/history.csv\", index=False, encoding='utf8')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Results Graph**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nfor c in ['train_loss', 'valid_loss']:\n    plt.plot(history[c], label=c)\nplt.legend()\nplt.xlabel('Epoch',color='k')\nplt.ylabel('Average Negative Log Likelihood')\nplt.title('Training and Validation Losses')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Save Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_checkpoint(model, path):\n\n    # Basic details\n    checkpoint = {\n                'epochs': model.epochs\n                 }\n\n    # Extract the final classifier and the state dictionary\n    checkpoint['fc'] = model.fc\n    checkpoint['state_dict'] = model.state_dict()\n\n    # Add the optimizer\n    checkpoint['optimizer'] = model.optimizer\n    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()\n    # Save the data to the path\n    torch.save(checkpoint, path)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_checkpoint(model, path=checkpoint_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Test the Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef check_mse_on_test(model, testloader, criterion=None, device='cpu'):  \n    \"\"\"\n    Compute the Mean Square Error on the test dataset\n    :param model - a pretrained model object\n    :param testloader - a generator object representing the test dataset\n    :param criterion - loss object\n    :param device - a string specifying whether to use cuda or cpu\n    return - MSE of predicted head circumference\n    \"\"\"\n    loss = 0\n    test_loss = 0\n    std_mean = 0\n    model.eval()\n    test_dataframe = pd.read_csv(test_filename)\n    with torch.no_grad():\n        for data,target in testloader:\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n            output = model(data)\n\n            test_dataframe ['predicted output'] = np.squeeze(output.cpu().numpy())\n            test_dataframe.to_csv(\"/kaggle/working/pred_test_file.csv\", index=False, encoding='utf8')\n\n            std_mean = torch.std_mean(output)\n            loss = criterion(output, target.float())\n            # Multiply average loss times the number of examples in batch\n            test_loss += loss.item() * data.size(0)\n            \n    return std_mean , test_loss / len(testloader.dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.L1Loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_mean , test_loss = check_mse_on_test(model, dataloaders['test'], criterion, device='cuda')\nprint('Test set MAE loss is {:.4f} Normalized HC'.format(test_loss))\nprint('Test set Standard deviation is {:.4f} Normalized HC'.format(std_mean[0] * 346.4))\nprint('Test set MAE loss is {:.4f} HC(mm)'.format(test_loss * 346.4))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}